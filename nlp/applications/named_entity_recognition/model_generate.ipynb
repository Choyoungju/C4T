{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdwn\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version :  1.10.0\n",
      "WARNING:tensorflow:From C:\\Users\\tjdwn\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From C:\\Users\\tjdwn\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "model/emb:0\n",
      "model/birnn/fw/gru_cell/gates/kernel:0\n",
      "model/birnn/fw/gru_cell/gates/bias:0\n",
      "model/birnn/fw/gru_cell/candidate/kernel:0\n",
      "model/birnn/fw/gru_cell/candidate/bias:0\n",
      "model/birnn/bw/gru_cell/gates/kernel:0\n",
      "model/birnn/bw/gru_cell/gates/bias:0\n",
      "model/birnn/bw/gru_cell/candidate/kernel:0\n",
      "model/birnn/bw/gru_cell/candidate/bias:0\n",
      "model/Rnn2Target/weights:0\n",
      "model/Rnn2Target/biases:0\n",
      "WARNING:tensorflow:From <ipython-input-1-5d709f19a05f>:187: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models\\model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:model/global_step/sec: 0\n",
      "Epoch =   1 Step =       1 loss = 2.559\n",
      "Epoch =   1 Step =       2 loss = 2.484\n",
      "Epoch =   1 Step =       3 loss = 2.338\n",
      "Epoch =   1 Step =       4 loss = 2.063\n",
      "Epoch =   1 Step =       5 loss = 1.941\n",
      "Epoch =   1 Step =       6 loss = 1.862\n",
      "Epoch =   1 Step =       7 loss = 1.760\n",
      "Epoch =   1 Step =       8 loss = 1.660\n",
      "Epoch =   2 Step =       9 loss = 1.583\n",
      "Epoch =   2 Step =      10 loss = 1.531\n",
      "Epoch =   3 Step =      20 loss = 1.234\n",
      "Epoch =   4 Step =      30 loss = 1.109\n",
      "Epoch =   5 Step =      40 loss = 1.021\n",
      "Epoch =   6 Step =      50 loss = 0.938\n",
      "Epoch =   7 Step =      60 loss = 0.866\n",
      "Epoch =   9 Step =      70 loss = 0.798\n",
      "Epoch =  10 Step =      80 loss = 0.737\n",
      "Epoch =  11 Step =      90 loss = 0.683\n",
      "Epoch =  12 Step =     100 loss = 0.636\n",
      "Epoch =  13 Step =     110 loss = 0.597\n",
      "Epoch =  14 Step =     120 loss = 0.563\n",
      "Epoch =  15 Step =     130 loss = 0.533\n",
      "Epoch =  17 Step =     140 loss = 0.506\n",
      "Epoch =  18 Step =     150 loss = 0.483\n",
      "Epoch =  19 Step =     160 loss = 0.461\n",
      "Epoch =  20 Step =     170 loss = 0.442\n",
      "Epoch =  21 Step =     180 loss = 0.425\n",
      "Epoch =  22 Step =     190 loss = 0.409\n",
      "Epoch =  23 Step =     200 loss = 0.394\n",
      "Epoch =  25 Step =     210 loss = 0.381\n",
      "Epoch =  26 Step =     220 loss = 0.368\n",
      "Epoch =  27 Step =     230 loss = 0.357\n",
      "Epoch =  28 Step =     240 loss = 0.346\n",
      "Epoch =  29 Step =     250 loss = 0.336\n",
      "Epoch =  30 Step =     260 loss = 0.326\n",
      "Epoch =  31 Step =     270 loss = 0.318\n",
      "Epoch =  33 Step =     280 loss = 0.309\n",
      "Epoch =  34 Step =     290 loss = 0.301\n",
      "Epoch =  35 Step =     300 loss = 0.294\n",
      "Epoch =  36 Step =     310 loss = 0.287\n",
      "Epoch =  37 Step =     320 loss = 0.280\n",
      "Epoch =  38 Step =     330 loss = 0.274\n",
      "Epoch =  39 Step =     340 loss = 0.268\n",
      "Epoch =  41 Step =     350 loss = 0.262\n",
      "Epoch =  42 Step =     360 loss = 0.257\n",
      "Epoch =  43 Step =     370 loss = 0.251\n",
      "Epoch =  44 Step =     380 loss = 0.246\n",
      "Epoch =  45 Step =     390 loss = 0.241\n",
      "Epoch =  46 Step =     400 loss = 0.237\n",
      "Epoch =  47 Step =     410 loss = 0.232\n",
      "Epoch =  49 Step =     420 loss = 0.228\n",
      "Epoch =  50 Step =     430 loss = 0.224\n",
      "Epoch =  51 Step =     440 loss = 0.220\n",
      "Epoch =  52 Step =     450 loss = 0.216\n",
      "Epoch =  53 Step =     460 loss = 0.212\n",
      "Epoch =  54 Step =     470 loss = 0.209\n",
      "Epoch =  55 Step =     480 loss = 0.205\n",
      "Epoch =  57 Step =     490 loss = 0.202\n",
      "Epoch =  58 Step =     500 loss = 0.199\n",
      "Epoch =  59 Step =     510 loss = 0.196\n",
      "INFO:tensorflow:model/global_step/sec: 4.31634\n",
      "Epoch =  60 Step =     520 loss = 0.193\n",
      "Epoch =  61 Step =     530 loss = 0.190\n",
      "Epoch =  62 Step =     540 loss = 0.187\n",
      "Epoch =  63 Step =     550 loss = 0.184\n",
      "Epoch =  65 Step =     560 loss = 0.181\n",
      "Epoch =  66 Step =     570 loss = 0.179\n",
      "Epoch =  67 Step =     580 loss = 0.176\n",
      "Epoch =  68 Step =     590 loss = 0.174\n",
      "Epoch =  69 Step =     600 loss = 0.171\n",
      "Epoch =  70 Step =     610 loss = 0.169\n",
      "Epoch =  71 Step =     620 loss = 0.167\n",
      "Epoch =  73 Step =     630 loss = 0.165\n",
      "Epoch =  74 Step =     640 loss = 0.163\n",
      "Epoch =  75 Step =     650 loss = 0.160\n",
      "Epoch =  76 Step =     660 loss = 0.158\n",
      "Epoch =  77 Step =     670 loss = 0.156\n",
      "Epoch =  78 Step =     680 loss = 0.154\n",
      "Epoch =  79 Step =     690 loss = 0.153\n",
      "Epoch =  81 Step =     700 loss = 0.151\n",
      "Epoch =  82 Step =     710 loss = 0.149\n",
      "Epoch =  83 Step =     720 loss = 0.147\n",
      "Epoch =  84 Step =     730 loss = 0.145\n",
      "Epoch =  85 Step =     740 loss = 0.144\n",
      "Epoch =  86 Step =     750 loss = 0.142\n",
      "Epoch =  87 Step =     760 loss = 0.141\n",
      "Epoch =  89 Step =     770 loss = 0.139\n",
      "Epoch =  90 Step =     780 loss = 0.137\n",
      "Epoch =  91 Step =     790 loss = 0.136\n",
      "Epoch =  92 Step =     800 loss = 0.135\n",
      "Epoch =  93 Step =     810 loss = 0.133\n",
      "Epoch =  94 Step =     820 loss = 0.132\n",
      "Epoch =  95 Step =     830 loss = 0.130\n",
      "Epoch =  97 Step =     840 loss = 0.129\n",
      "Epoch =  98 Step =     850 loss = 0.128\n",
      "Epoch =  99 Step =     860 loss = 0.127\n",
      "Epoch = 100 Step =     870 loss = 0.125\n",
      "Epoch = 101 Step =     880 loss = 0.124\n",
      "Epoch = 102 Step =     890 loss = 0.123\n",
      "Epoch = 103 Step =     900 loss = 0.122\n",
      "Epoch = 105 Step =     910 loss = 0.120\n",
      "Epoch = 106 Step =     920 loss = 0.119\n",
      "Epoch = 107 Step =     930 loss = 0.118\n",
      "Epoch = 108 Step =     940 loss = 0.117\n",
      "Epoch = 109 Step =     950 loss = 0.116\n",
      "Epoch = 110 Step =     960 loss = 0.115\n",
      "Epoch = 111 Step =     970 loss = 0.114\n",
      "Epoch = 113 Step =     980 loss = 0.113\n",
      "Epoch = 114 Step =     990 loss = 0.112\n",
      "Epoch = 115 Step =    1000 loss = 0.111\n",
      "Epoch = 116 Step =    1010 loss = 0.110\n",
      "Epoch = 117 Step =    1020 loss = 0.109\n",
      "Epoch = 118 Step =    1030 loss = 0.108\n",
      "Epoch = 119 Step =    1040 loss = 0.107\n",
      "Epoch = 121 Step =    1050 loss = 0.106\n",
      "Epoch = 122 Step =    1060 loss = 0.105\n",
      "Epoch = 123 Step =    1070 loss = 0.105\n",
      "INFO:tensorflow:model/global_step/sec: 4.61681\n",
      "Epoch = 124 Step =    1080 loss = 0.104\n",
      "Epoch = 125 Step =    1090 loss = 0.103\n",
      "Epoch = 126 Step =    1100 loss = 0.102\n",
      "Epoch = 128 Step =    1110 loss = 0.101\n",
      "Epoch = 129 Step =    1120 loss = 0.100\n",
      "Epoch = 130 Step =    1130 loss = 0.100\n",
      "Epoch = 131 Step =    1140 loss = 0.099\n",
      "Epoch = 132 Step =    1150 loss = 0.098\n",
      "Epoch = 133 Step =    1160 loss = 0.097\n",
      "Epoch = 134 Step =    1170 loss = 0.097\n",
      "Epoch = 136 Step =    1180 loss = 0.096\n",
      "Epoch = 137 Step =    1190 loss = 0.095\n",
      "Epoch = 138 Step =    1200 loss = 0.095\n",
      "Epoch = 139 Step =    1210 loss = 0.094\n",
      "Epoch = 140 Step =    1220 loss = 0.093\n",
      "Epoch = 141 Step =    1230 loss = 0.093\n",
      "Epoch = 142 Step =    1240 loss = 0.092\n",
      "Epoch = 144 Step =    1250 loss = 0.091\n",
      "Epoch = 145 Step =    1260 loss = 0.091\n",
      "Epoch = 146 Step =    1270 loss = 0.090\n",
      "Epoch = 147 Step =    1280 loss = 0.089\n",
      "Epoch = 148 Step =    1290 loss = 0.089\n",
      "Epoch = 149 Step =    1300 loss = 0.088\n",
      "Epoch = 150 Step =    1310 loss = 0.088\n",
      "Epoch = 152 Step =    1320 loss = 0.087\n",
      "Epoch = 153 Step =    1330 loss = 0.086\n",
      "Epoch = 154 Step =    1340 loss = 0.086\n",
      "Epoch = 155 Step =    1350 loss = 0.085\n",
      "Epoch = 156 Step =    1360 loss = 0.085\n",
      "Epoch = 157 Step =    1370 loss = 0.084\n",
      "Epoch = 158 Step =    1380 loss = 0.084\n",
      "Epoch = 160 Step =    1390 loss = 0.083\n",
      "Epoch = 161 Step =    1400 loss = 0.083\n",
      "Epoch = 162 Step =    1410 loss = 0.082\n",
      "Epoch = 163 Step =    1420 loss = 0.081\n",
      "Epoch = 164 Step =    1430 loss = 0.081\n",
      "Epoch = 165 Step =    1440 loss = 0.080\n",
      "Epoch = 166 Step =    1450 loss = 0.080\n",
      "Epoch = 168 Step =    1460 loss = 0.079\n",
      "Epoch = 169 Step =    1470 loss = 0.079\n",
      "Epoch = 170 Step =    1480 loss = 0.078\n",
      "Epoch = 171 Step =    1490 loss = 0.078\n",
      "Epoch = 172 Step =    1500 loss = 0.078\n",
      "Epoch = 173 Step =    1510 loss = 0.077\n",
      "Epoch = 174 Step =    1520 loss = 0.077\n",
      "Epoch = 176 Step =    1530 loss = 0.076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 177 Step =    1540 loss = 0.076\n",
      "Epoch = 178 Step =    1550 loss = 0.075\n",
      "Epoch = 179 Step =    1560 loss = 0.075\n",
      "Epoch = 180 Step =    1570 loss = 0.074\n",
      "Epoch = 181 Step =    1580 loss = 0.074\n",
      "Epoch = 182 Step =    1590 loss = 0.074\n",
      "Epoch = 184 Step =    1600 loss = 0.073\n",
      "Epoch = 185 Step =    1610 loss = 0.073\n",
      "Epoch = 186 Step =    1620 loss = 0.072\n",
      "INFO:tensorflow:model/global_step/sec: 4.60003\n",
      "Epoch = 187 Step =    1630 loss = 0.072\n",
      "Epoch = 188 Step =    1640 loss = 0.072\n",
      "Epoch = 189 Step =    1650 loss = 0.071\n",
      "Epoch = 190 Step =    1660 loss = 0.071\n",
      "Epoch = 192 Step =    1670 loss = 0.070\n",
      "Epoch = 193 Step =    1680 loss = 0.070\n",
      "Epoch = 194 Step =    1690 loss = 0.070\n",
      "Epoch = 195 Step =    1700 loss = 0.069\n",
      "Epoch = 196 Step =    1710 loss = 0.069\n",
      "Epoch = 197 Step =    1720 loss = 0.069\n",
      "Epoch = 198 Step =    1730 loss = 0.068\n",
      "Epoch = 200 Step =    1740 loss = 0.068\n",
      "Epoch = 201 Step =    1750 loss = 0.068\n",
      "Epoch = 202 Step =    1760 loss = 0.067\n",
      "Epoch = 203 Step =    1770 loss = 0.067\n",
      "Epoch = 204 Step =    1780 loss = 0.067\n",
      "Epoch = 205 Step =    1790 loss = 0.066\n",
      "Epoch = 206 Step =    1800 loss = 0.066\n",
      "Epoch = 208 Step =    1810 loss = 0.066\n",
      "Epoch = 209 Step =    1820 loss = 0.065\n",
      "Epoch = 210 Step =    1830 loss = 0.065\n",
      "Epoch = 211 Step =    1840 loss = 0.065\n",
      "Epoch = 212 Step =    1850 loss = 0.064\n",
      "Epoch = 213 Step =    1860 loss = 0.064\n",
      "Epoch = 214 Step =    1870 loss = 0.064\n",
      "Epoch = 216 Step =    1880 loss = 0.063\n",
      "Epoch = 217 Step =    1890 loss = 0.063\n",
      "Epoch = 218 Step =    1900 loss = 0.063\n",
      "Epoch = 219 Step =    1910 loss = 0.062\n",
      "Epoch = 220 Step =    1920 loss = 0.062\n",
      "Epoch = 221 Step =    1930 loss = 0.062\n",
      "Epoch = 222 Step =    1940 loss = 0.062\n",
      "Epoch = 224 Step =    1950 loss = 0.061\n",
      "Epoch = 225 Step =    1960 loss = 0.061\n",
      "Epoch = 226 Step =    1970 loss = 0.061\n",
      "Epoch = 227 Step =    1980 loss = 0.061\n",
      "Epoch = 228 Step =    1990 loss = 0.060\n",
      "Epoch = 229 Step =    2000 loss = 0.060\n",
      "Epoch = 230 Step =    2010 loss = 0.060\n",
      "Epoch = 232 Step =    2020 loss = 0.059\n",
      "Epoch = 233 Step =    2030 loss = 0.059\n",
      "Epoch = 234 Step =    2040 loss = 0.059\n",
      "Epoch = 235 Step =    2050 loss = 0.059\n",
      "Epoch = 236 Step =    2060 loss = 0.058\n",
      "Epoch = 237 Step =    2070 loss = 0.058\n",
      "Epoch = 238 Step =    2080 loss = 0.058\n",
      "Epoch = 240 Step =    2090 loss = 0.058\n",
      "Epoch = 241 Step =    2100 loss = 0.057\n",
      "Epoch = 242 Step =    2110 loss = 0.057\n",
      "Epoch = 243 Step =    2120 loss = 0.057\n",
      "Epoch = 244 Step =    2130 loss = 0.057\n",
      "Epoch = 245 Step =    2140 loss = 0.056\n",
      "Epoch = 246 Step =    2150 loss = 0.056\n",
      "Epoch = 248 Step =    2160 loss = 0.056\n",
      "Epoch = 249 Step =    2170 loss = 0.056\n",
      "INFO:tensorflow:model/global_step/sec: 4.58329\n",
      "Epoch = 250 Step =    2180 loss = 0.056\n",
      "Epoch = 251 Step =    2190 loss = 0.055\n",
      "Epoch = 252 Step =    2200 loss = 0.055\n",
      "Epoch = 253 Step =    2210 loss = 0.055\n",
      "Epoch = 255 Step =    2220 loss = 0.055\n",
      "Epoch = 256 Step =    2230 loss = 0.054\n",
      "Epoch = 257 Step =    2240 loss = 0.054\n",
      "Epoch = 258 Step =    2250 loss = 0.054\n",
      "Epoch = 259 Step =    2260 loss = 0.054\n",
      "Epoch = 260 Step =    2270 loss = 0.054\n",
      "Epoch = 261 Step =    2280 loss = 0.053\n",
      "Epoch = 263 Step =    2290 loss = 0.053\n",
      "Epoch = 264 Step =    2300 loss = 0.053\n",
      "Epoch = 265 Step =    2310 loss = 0.053\n",
      "Epoch = 266 Step =    2320 loss = 0.053\n",
      "Epoch = 267 Step =    2330 loss = 0.052\n",
      "Epoch = 268 Step =    2340 loss = 0.052\n",
      "Epoch = 269 Step =    2350 loss = 0.052\n",
      "Epoch = 271 Step =    2360 loss = 0.052\n",
      "Epoch = 272 Step =    2370 loss = 0.052\n",
      "Epoch = 273 Step =    2380 loss = 0.051\n",
      "Epoch = 274 Step =    2390 loss = 0.051\n",
      "Epoch = 275 Step =    2400 loss = 0.051\n",
      "Epoch = 276 Step =    2410 loss = 0.051\n",
      "Epoch = 277 Step =    2420 loss = 0.051\n",
      "Epoch = 279 Step =    2430 loss = 0.050\n",
      "Epoch = 280 Step =    2440 loss = 0.050\n",
      "Epoch = 281 Step =    2450 loss = 0.050\n",
      "Epoch = 282 Step =    2460 loss = 0.050\n",
      "Epoch = 283 Step =    2470 loss = 0.050\n",
      "Epoch = 284 Step =    2480 loss = 0.050\n",
      "Epoch = 285 Step =    2490 loss = 0.049\n",
      "Epoch = 287 Step =    2500 loss = 0.049\n",
      "Epoch = 288 Step =    2510 loss = 0.049\n",
      "Epoch = 289 Step =    2520 loss = 0.049\n",
      "Epoch = 290 Step =    2530 loss = 0.049\n",
      "Epoch = 291 Step =    2540 loss = 0.048\n",
      "Epoch = 292 Step =    2550 loss = 0.048\n",
      "Epoch = 293 Step =    2560 loss = 0.048\n",
      "Epoch = 295 Step =    2570 loss = 0.048\n",
      "Epoch = 296 Step =    2580 loss = 0.048\n",
      "Epoch = 297 Step =    2590 loss = 0.048\n",
      "Epoch = 298 Step =    2600 loss = 0.047\n",
      "Epoch = 299 Step =    2610 loss = 0.047\n",
      "Epoch = 300 Step =    2620 loss = 0.047\n",
      "Epoch = 301 Step =    2630 loss = 0.047\n",
      "Epoch = 303 Step =    2640 loss = 0.047\n",
      "Epoch = 304 Step =    2650 loss = 0.047\n",
      "Epoch = 305 Step =    2660 loss = 0.047\n",
      "Epoch = 306 Step =    2670 loss = 0.046\n",
      "Epoch = 307 Step =    2680 loss = 0.046\n",
      "Epoch = 308 Step =    2690 loss = 0.046\n",
      "Epoch = 309 Step =    2700 loss = 0.046\n",
      "Epoch = 311 Step =    2710 loss = 0.046\n",
      "Epoch = 312 Step =    2720 loss = 0.046\n",
      "INFO:tensorflow:model/global_step/sec: 4.57506\n",
      "INFO:tensorflow:Saving checkpoint to path ./trained_models\\model.ckpt\n",
      "Epoch = 313 Step =    2730 loss = 0.045\n",
      "Epoch = 314 Step =    2740 loss = 0.045\n",
      "Epoch = 315 Step =    2750 loss = 0.045\n",
      "Epoch = 316 Step =    2760 loss = 0.045\n",
      "Epoch = 317 Step =    2770 loss = 0.045\n",
      "Epoch = 319 Step =    2780 loss = 0.045\n",
      "Epoch = 320 Step =    2790 loss = 0.045\n",
      "Epoch = 321 Step =    2800 loss = 0.044\n",
      "Epoch = 322 Step =    2810 loss = 0.044\n",
      "Epoch = 323 Step =    2820 loss = 0.044\n",
      "Epoch = 324 Step =    2830 loss = 0.044\n",
      "Epoch = 325 Step =    2840 loss = 0.044\n",
      "Epoch = 327 Step =    2850 loss = 0.044\n",
      "Epoch = 328 Step =    2860 loss = 0.044\n",
      "Epoch = 329 Step =    2870 loss = 0.044\n",
      "Epoch = 330 Step =    2880 loss = 0.043\n",
      "Epoch = 331 Step =    2890 loss = 0.043\n",
      "Epoch = 332 Step =    2900 loss = 0.043\n",
      "Epoch = 333 Step =    2910 loss = 0.043\n",
      "Epoch = 335 Step =    2920 loss = 0.043\n",
      "Epoch = 336 Step =    2930 loss = 0.043\n",
      "Epoch = 337 Step =    2940 loss = 0.043\n",
      "Epoch = 338 Step =    2950 loss = 0.042\n",
      "Epoch = 339 Step =    2960 loss = 0.042\n",
      "Epoch = 340 Step =    2970 loss = 0.042\n",
      "Epoch = 341 Step =    2980 loss = 0.042\n",
      "Epoch = 343 Step =    2990 loss = 0.042\n",
      "Epoch = 344 Step =    3000 loss = 0.042\n",
      "Epoch = 345 Step =    3010 loss = 0.042\n",
      "Epoch = 346 Step =    3020 loss = 0.042\n",
      "Epoch = 347 Step =    3030 loss = 0.042\n",
      "Epoch = 348 Step =    3040 loss = 0.041\n",
      "Epoch = 349 Step =    3050 loss = 0.041\n",
      "Epoch = 351 Step =    3060 loss = 0.041\n",
      "Epoch = 352 Step =    3070 loss = 0.041\n",
      "Epoch = 353 Step =    3080 loss = 0.041\n",
      "Epoch = 354 Step =    3090 loss = 0.041\n",
      "Epoch = 355 Step =    3100 loss = 0.041\n",
      "Epoch = 356 Step =    3110 loss = 0.041\n",
      "Epoch = 357 Step =    3120 loss = 0.041\n",
      "Epoch = 359 Step =    3130 loss = 0.040\n",
      "Epoch = 360 Step =    3140 loss = 0.040\n",
      "Epoch = 361 Step =    3150 loss = 0.040\n",
      "Epoch = 362 Step =    3160 loss = 0.040\n",
      "Epoch = 363 Step =    3170 loss = 0.040\n",
      "Epoch = 364 Step =    3180 loss = 0.040\n",
      "Epoch = 365 Step =    3190 loss = 0.040\n",
      "Epoch = 367 Step =    3200 loss = 0.040\n",
      "Epoch = 368 Step =    3210 loss = 0.040\n",
      "Epoch = 369 Step =    3220 loss = 0.039\n",
      "Epoch = 370 Step =    3230 loss = 0.039\n",
      "Epoch = 371 Step =    3240 loss = 0.039\n",
      "Epoch = 372 Step =    3250 loss = 0.039\n",
      "Epoch = 373 Step =    3260 loss = 0.039\n",
      "Epoch = 375 Step =    3270 loss = 0.039\n",
      "INFO:tensorflow:model/global_step/sec: 4.6\n",
      "Epoch = 376 Step =    3280 loss = 0.039\n",
      "Epoch = 377 Step =    3290 loss = 0.039\n",
      "Epoch = 378 Step =    3300 loss = 0.039\n",
      "Epoch = 379 Step =    3310 loss = 0.038\n",
      "Epoch = 380 Step =    3320 loss = 0.038\n",
      "Epoch = 382 Step =    3330 loss = 0.038\n",
      "Epoch = 383 Step =    3340 loss = 0.038\n",
      "Epoch = 384 Step =    3350 loss = 0.038\n",
      "Epoch = 385 Step =    3360 loss = 0.038\n",
      "Epoch = 386 Step =    3370 loss = 0.038\n",
      "Epoch = 387 Step =    3380 loss = 0.038\n",
      "Epoch = 388 Step =    3390 loss = 0.038\n",
      "Epoch = 390 Step =    3400 loss = 0.038\n",
      "Epoch = 391 Step =    3410 loss = 0.037\n",
      "Epoch = 392 Step =    3420 loss = 0.037\n",
      "Epoch = 393 Step =    3430 loss = 0.037\n",
      "Epoch = 394 Step =    3440 loss = 0.037\n",
      "Epoch = 395 Step =    3450 loss = 0.037\n",
      "Epoch = 396 Step =    3460 loss = 0.037\n",
      "Epoch = 398 Step =    3470 loss = 0.037\n",
      "Epoch = 399 Step =    3480 loss = 0.037\n",
      "Epoch = 400 Step =    3490 loss = 0.037\n",
      "Epoch = 401 Step =    3500 loss = 0.037\n",
      "Epoch = 402 Step =    3510 loss = 0.037\n",
      "Epoch = 403 Step =    3520 loss = 0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 404 Step =    3530 loss = 0.036\n",
      "Epoch = 406 Step =    3540 loss = 0.036\n",
      "Epoch = 407 Step =    3550 loss = 0.036\n",
      "Epoch = 408 Step =    3560 loss = 0.036\n",
      "Epoch = 409 Step =    3570 loss = 0.036\n",
      "Epoch = 410 Step =    3580 loss = 0.036\n",
      "Epoch = 411 Step =    3590 loss = 0.036\n",
      "Epoch = 412 Step =    3600 loss = 0.036\n",
      "Epoch = 414 Step =    3610 loss = 0.036\n",
      "Epoch = 415 Step =    3620 loss = 0.036\n",
      "Epoch = 416 Step =    3630 loss = 0.036\n",
      "Epoch = 417 Step =    3640 loss = 0.035\n",
      "Epoch = 418 Step =    3650 loss = 0.035\n",
      "Epoch = 419 Step =    3660 loss = 0.035\n",
      "Epoch = 420 Step =    3670 loss = 0.035\n",
      "Epoch = 422 Step =    3680 loss = 0.035\n",
      "Epoch = 423 Step =    3690 loss = 0.035\n",
      "Epoch = 424 Step =    3700 loss = 0.035\n",
      "Epoch = 425 Step =    3710 loss = 0.035\n",
      "Epoch = 426 Step =    3720 loss = 0.035\n",
      "Epoch = 427 Step =    3730 loss = 0.035\n",
      "Epoch = 428 Step =    3740 loss = 0.035\n",
      "Epoch = 430 Step =    3750 loss = 0.035\n",
      "Epoch = 431 Step =    3760 loss = 0.035\n",
      "Epoch = 432 Step =    3770 loss = 0.034\n",
      "Epoch = 433 Step =    3780 loss = 0.034\n",
      "Epoch = 434 Step =    3790 loss = 0.034\n",
      "Epoch = 435 Step =    3800 loss = 0.034\n",
      "Epoch = 436 Step =    3810 loss = 0.034\n",
      "Epoch = 438 Step =    3820 loss = 0.034\n",
      "Epoch = 439 Step =    3830 loss = 0.034\n",
      "Epoch = 440 Step =    3840 loss = 0.034\n",
      "INFO:tensorflow:model/global_step/sec: 4.73297\n",
      "Epoch = 441 Step =    3850 loss = 0.034\n",
      "Epoch = 442 Step =    3860 loss = 0.034\n",
      "Epoch = 443 Step =    3870 loss = 0.034\n",
      "Epoch = 444 Step =    3880 loss = 0.034\n",
      "Epoch = 446 Step =    3890 loss = 0.034\n",
      "Epoch = 447 Step =    3900 loss = 0.034\n",
      "Epoch = 448 Step =    3910 loss = 0.033\n",
      "Epoch = 449 Step =    3920 loss = 0.033\n",
      "Epoch = 450 Step =    3930 loss = 0.033\n",
      "Epoch = 451 Step =    3940 loss = 0.033\n",
      "Epoch = 452 Step =    3950 loss = 0.033\n",
      "Epoch = 454 Step =    3960 loss = 0.033\n",
      "Epoch = 455 Step =    3970 loss = 0.033\n",
      "Epoch = 456 Step =    3980 loss = 0.033\n",
      "Epoch = 457 Step =    3990 loss = 0.033\n",
      "Epoch = 458 Step =    4000 loss = 0.033\n",
      "Epoch = 459 Step =    4010 loss = 0.033\n",
      "Epoch = 460 Step =    4020 loss = 0.033\n",
      "Epoch = 462 Step =    4030 loss = 0.033\n",
      "Epoch = 463 Step =    4040 loss = 0.033\n",
      "Epoch = 464 Step =    4050 loss = 0.033\n",
      "Epoch = 465 Step =    4060 loss = 0.032\n",
      "Epoch = 466 Step =    4070 loss = 0.032\n",
      "Epoch = 467 Step =    4080 loss = 0.032\n",
      "Epoch = 468 Step =    4090 loss = 0.032\n",
      "Epoch = 470 Step =    4100 loss = 0.032\n",
      "Epoch = 471 Step =    4110 loss = 0.032\n",
      "Epoch = 472 Step =    4120 loss = 0.032\n",
      "Epoch = 473 Step =    4130 loss = 0.032\n",
      "Epoch = 474 Step =    4140 loss = 0.032\n",
      "Epoch = 475 Step =    4150 loss = 0.032\n",
      "Epoch = 476 Step =    4160 loss = 0.032\n",
      "Epoch = 478 Step =    4170 loss = 0.032\n",
      "Epoch = 479 Step =    4180 loss = 0.032\n",
      "Epoch = 480 Step =    4190 loss = 0.032\n",
      "Epoch = 481 Step =    4200 loss = 0.032\n",
      "Epoch = 482 Step =    4210 loss = 0.031\n",
      "Epoch = 483 Step =    4220 loss = 0.031\n",
      "Epoch = 484 Step =    4230 loss = 0.031\n",
      "Epoch = 486 Step =    4240 loss = 0.031\n",
      "Epoch = 487 Step =    4250 loss = 0.031\n",
      "Epoch = 488 Step =    4260 loss = 0.031\n",
      "Epoch = 489 Step =    4270 loss = 0.031\n",
      "Epoch = 490 Step =    4280 loss = 0.031\n",
      "Epoch = 491 Step =    4290 loss = 0.031\n",
      "Epoch = 492 Step =    4300 loss = 0.031\n",
      "Epoch = 494 Step =    4310 loss = 0.031\n",
      "Epoch = 495 Step =    4320 loss = 0.031\n",
      "Epoch = 496 Step =    4330 loss = 0.031\n",
      "Epoch = 497 Step =    4340 loss = 0.031\n",
      "Epoch = 498 Step =    4350 loss = 0.031\n",
      "Epoch = 499 Step =    4360 loss = 0.031\n",
      "Epoch = 500 Step =    4370 loss = 0.030\n",
      "Epoch = 502 Step =    4380 loss = 0.030\n",
      "Epoch = 503 Step =    4390 loss = 0.030\n",
      "Epoch = 504 Step =    4400 loss = 0.030\n",
      "Epoch = 505 Step =    4410 loss = 0.030\n",
      "INFO:tensorflow:model/global_step/sec: 4.73345\n",
      "Epoch = 506 Step =    4420 loss = 0.030\n",
      "Epoch = 507 Step =    4430 loss = 0.030\n",
      "Epoch = 509 Step =    4440 loss = 0.030\n",
      "Epoch = 510 Step =    4450 loss = 0.030\n",
      "Epoch = 511 Step =    4460 loss = 0.030\n",
      "Epoch = 512 Step =    4470 loss = 0.030\n",
      "Epoch = 513 Step =    4480 loss = 0.030\n",
      "Epoch = 514 Step =    4490 loss = 0.030\n",
      "Epoch = 515 Step =    4500 loss = 0.030\n",
      "Epoch = 517 Step =    4510 loss = 0.030\n",
      "Epoch = 518 Step =    4520 loss = 0.030\n",
      "Epoch = 519 Step =    4530 loss = 0.030\n",
      "Epoch = 520 Step =    4540 loss = 0.030\n",
      "Epoch = 521 Step =    4550 loss = 0.029\n",
      "Epoch = 522 Step =    4560 loss = 0.029\n",
      "Epoch = 523 Step =    4570 loss = 0.029\n",
      "Epoch = 525 Step =    4580 loss = 0.029\n",
      "Epoch = 526 Step =    4590 loss = 0.029\n",
      "Epoch = 527 Step =    4600 loss = 0.029\n",
      "Epoch = 528 Step =    4610 loss = 0.029\n",
      "Epoch = 529 Step =    4620 loss = 0.029\n",
      "Epoch = 530 Step =    4630 loss = 0.029\n",
      "Epoch = 531 Step =    4640 loss = 0.029\n",
      "Epoch = 533 Step =    4650 loss = 0.029\n",
      "Epoch = 534 Step =    4660 loss = 0.029\n",
      "Epoch = 535 Step =    4670 loss = 0.029\n",
      "Epoch = 536 Step =    4680 loss = 0.029\n",
      "Epoch = 537 Step =    4690 loss = 0.029\n",
      "Epoch = 538 Step =    4700 loss = 0.029\n",
      "Epoch = 539 Step =    4710 loss = 0.029\n",
      "Epoch = 541 Step =    4720 loss = 0.029\n",
      "Epoch = 542 Step =    4730 loss = 0.029\n",
      "Epoch = 543 Step =    4740 loss = 0.029\n",
      "Epoch = 544 Step =    4750 loss = 0.029\n",
      "Epoch = 545 Step =    4760 loss = 0.028\n",
      "Epoch = 546 Step =    4770 loss = 0.028\n",
      "Epoch = 547 Step =    4780 loss = 0.028\n",
      "Epoch = 549 Step =    4790 loss = 0.028\n",
      "Epoch = 550 Step =    4800 loss = 0.028\n",
      "Epoch = 551 Step =    4810 loss = 0.028\n",
      "Epoch = 552 Step =    4820 loss = 0.028\n",
      "Epoch = 553 Step =    4830 loss = 0.028\n",
      "Epoch = 554 Step =    4840 loss = 0.028\n",
      "Epoch = 555 Step =    4850 loss = 0.028\n",
      "Epoch = 557 Step =    4860 loss = 0.028\n",
      "Epoch = 558 Step =    4870 loss = 0.028\n",
      "Epoch = 559 Step =    4880 loss = 0.028\n",
      "Epoch = 560 Step =    4890 loss = 0.028\n",
      "Epoch = 561 Step =    4900 loss = 0.028\n",
      "Epoch = 562 Step =    4910 loss = 0.028\n",
      "Epoch = 563 Step =    4920 loss = 0.028\n",
      "Epoch = 565 Step =    4930 loss = 0.028\n",
      "Epoch = 566 Step =    4940 loss = 0.028\n",
      "Epoch = 567 Step =    4950 loss = 0.028\n",
      "Epoch = 568 Step =    4960 loss = 0.028\n",
      "Epoch = 569 Step =    4970 loss = 0.028\n",
      "INFO:tensorflow:model/global_step/sec: 4.73384\n",
      "Epoch = 570 Step =    4980 loss = 0.027\n",
      "Epoch = 571 Step =    4990 loss = 0.027\n",
      "Epoch = 573 Step =    5000 loss = 0.027\n",
      "Epoch = 574 Step =    5010 loss = 0.027\n",
      "Epoch = 575 Step =    5020 loss = 0.027\n",
      "Epoch = 576 Step =    5030 loss = 0.027\n",
      "Epoch = 577 Step =    5040 loss = 0.027\n",
      "Epoch = 578 Step =    5050 loss = 0.027\n",
      "Epoch = 579 Step =    5060 loss = 0.027\n",
      "Epoch = 581 Step =    5070 loss = 0.027\n",
      "Epoch = 582 Step =    5080 loss = 0.027\n",
      "Epoch = 583 Step =    5090 loss = 0.027\n",
      "Epoch = 584 Step =    5100 loss = 0.027\n",
      "Epoch = 585 Step =    5110 loss = 0.027\n",
      "Epoch = 586 Step =    5120 loss = 0.027\n",
      "Epoch = 587 Step =    5130 loss = 0.027\n",
      "Epoch = 589 Step =    5140 loss = 0.027\n",
      "Epoch = 590 Step =    5150 loss = 0.027\n",
      "Epoch = 591 Step =    5160 loss = 0.027\n",
      "Epoch = 592 Step =    5170 loss = 0.027\n",
      "Epoch = 593 Step =    5180 loss = 0.027\n",
      "Epoch = 594 Step =    5190 loss = 0.027\n",
      "Epoch = 595 Step =    5200 loss = 0.027\n",
      "Epoch = 597 Step =    5210 loss = 0.027\n",
      "Epoch = 598 Step =    5220 loss = 0.027\n",
      "Epoch = 599 Step =    5230 loss = 0.026\n",
      "Epoch = 600 Step =    5240 loss = 0.026\n",
      "Epoch = 601 Step =    5250 loss = 0.026\n",
      "Epoch = 602 Step =    5260 loss = 0.026\n",
      "Epoch = 603 Step =    5270 loss = 0.026\n",
      "Epoch = 605 Step =    5280 loss = 0.026\n",
      "Epoch = 606 Step =    5290 loss = 0.026\n",
      "Epoch = 607 Step =    5300 loss = 0.026\n",
      "Epoch = 608 Step =    5310 loss = 0.026\n",
      "Epoch = 609 Step =    5320 loss = 0.026\n",
      "Epoch = 610 Step =    5330 loss = 0.026\n",
      "Epoch = 611 Step =    5340 loss = 0.026\n",
      "Epoch = 613 Step =    5350 loss = 0.026\n",
      "Epoch = 614 Step =    5360 loss = 0.026\n",
      "Epoch = 615 Step =    5370 loss = 0.026\n",
      "Epoch = 616 Step =    5380 loss = 0.026\n",
      "Epoch = 617 Step =    5390 loss = 0.026\n",
      "Epoch = 618 Step =    5400 loss = 0.026\n",
      "Epoch = 619 Step =    5410 loss = 0.026\n",
      "Epoch = 621 Step =    5420 loss = 0.026\n",
      "Epoch = 622 Step =    5430 loss = 0.026\n",
      "Epoch = 623 Step =    5440 loss = 0.026\n",
      "Epoch = 624 Step =    5450 loss = 0.026\n",
      "Epoch = 625 Step =    5460 loss = 0.026\n",
      "Epoch = 626 Step =    5470 loss = 0.026\n",
      "Epoch = 628 Step =    5480 loss = 0.026\n",
      "Epoch = 629 Step =    5490 loss = 0.026\n",
      "Epoch = 630 Step =    5500 loss = 0.025\n",
      "Epoch = 631 Step =    5510 loss = 0.025\n",
      "Epoch = 632 Step =    5520 loss = 0.025\n",
      "Epoch = 633 Step =    5530 loss = 0.025\n",
      "Epoch = 634 Step =    5540 loss = 0.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoint to path ./trained_models\\model.ckpt\n",
      "Epoch = 636 Step =    5550 loss = 0.025\n",
      "Epoch = 637 Step =    5560 loss = 0.025\n",
      "Epoch = 638 Step =    5570 loss = 0.025\n",
      "Epoch = 639 Step =    5580 loss = 0.025\n",
      "Epoch = 640 Step =    5590 loss = 0.025\n",
      "Epoch = 641 Step =    5600 loss = 0.025\n",
      "Epoch = 642 Step =    5610 loss = 0.025\n",
      "Epoch = 644 Step =    5620 loss = 0.025\n",
      "Epoch = 645 Step =    5630 loss = 0.025\n",
      "Epoch = 646 Step =    5640 loss = 0.025\n",
      "Epoch = 647 Step =    5650 loss = 0.025\n",
      "Epoch = 648 Step =    5660 loss = 0.025\n",
      "Epoch = 649 Step =    5670 loss = 0.025\n",
      "Epoch = 650 Step =    5680 loss = 0.025\n",
      "Epoch = 652 Step =    5690 loss = 0.025\n",
      "Epoch = 653 Step =    5700 loss = 0.025\n",
      "Epoch = 654 Step =    5710 loss = 0.025\n",
      "Epoch = 655 Step =    5720 loss = 0.025\n",
      "Epoch = 656 Step =    5730 loss = 0.025\n",
      "Epoch = 657 Step =    5740 loss = 0.025\n",
      "Epoch = 658 Step =    5750 loss = 0.025\n",
      "Epoch = 660 Step =    5760 loss = 0.025\n",
      "Epoch = 661 Step =    5770 loss = 0.025\n",
      "Epoch = 662 Step =    5780 loss = 0.024\n",
      "Epoch = 663 Step =    5790 loss = 0.024\n",
      "Epoch = 664 Step =    5800 loss = 0.024\n",
      "Epoch = 665 Step =    5810 loss = 0.024\n",
      "Epoch = 666 Step =    5820 loss = 0.024\n",
      "Epoch = 668 Step =    5830 loss = 0.024\n",
      "Epoch = 669 Step =    5840 loss = 0.024\n",
      "Epoch = 670 Step =    5850 loss = 0.024\n",
      "Epoch = 671 Step =    5860 loss = 0.024\n",
      "Epoch = 672 Step =    5870 loss = 0.024\n",
      "Epoch = 673 Step =    5880 loss = 0.024\n",
      "Epoch = 674 Step =    5890 loss = 0.024\n",
      "Epoch = 676 Step =    5900 loss = 0.024\n",
      "Epoch = 677 Step =    5910 loss = 0.024\n",
      "Epoch = 678 Step =    5920 loss = 0.024\n",
      "Epoch = 679 Step =    5930 loss = 0.024\n",
      "Epoch = 680 Step =    5940 loss = 0.024\n",
      "Epoch = 681 Step =    5950 loss = 0.024\n",
      "Epoch = 682 Step =    5960 loss = 0.024\n",
      "Epoch = 684 Step =    5970 loss = 0.024\n",
      "Epoch = 685 Step =    5980 loss = 0.024\n",
      "Epoch = 686 Step =    5990 loss = 0.024\n",
      "Epoch = 687 Step =    6000 loss = 0.024\n",
      "Epoch = 688 Step =    6010 loss = 0.024\n",
      "Epoch = 689 Step =    6020 loss = 0.024\n",
      "Epoch = 690 Step =    6030 loss = 0.024\n",
      "Epoch = 692 Step =    6040 loss = 0.024\n",
      "Epoch = 693 Step =    6050 loss = 0.024\n",
      "Epoch = 694 Step =    6060 loss = 0.024\n",
      "Epoch = 695 Step =    6070 loss = 0.024\n",
      "Epoch = 696 Step =    6080 loss = 0.024\n",
      "Epoch = 697 Step =    6090 loss = 0.024\n",
      "Epoch = 698 Step =    6100 loss = 0.023\n",
      "Epoch = 700 Step =    6110 loss = 0.023\n",
      "Epoch = 701 Step =    6120 loss = 0.023\n",
      "Epoch = 702 Step =    6130 loss = 0.023\n",
      "Epoch = 703 Step =    6140 loss = 0.023\n",
      "Epoch = 704 Step =    6150 loss = 0.023\n",
      "Epoch = 705 Step =    6160 loss = 0.023\n",
      "Epoch = 706 Step =    6170 loss = 0.023\n",
      "Epoch = 708 Step =    6180 loss = 0.023\n",
      "Epoch = 709 Step =    6190 loss = 0.023\n",
      "Epoch = 710 Step =    6200 loss = 0.023\n",
      "Epoch = 711 Step =    6210 loss = 0.023\n",
      "Epoch = 712 Step =    6220 loss = 0.023\n",
      "Epoch = 713 Step =    6230 loss = 0.023\n",
      "Epoch = 714 Step =    6240 loss = 0.023\n",
      "Epoch = 716 Step =    6250 loss = 0.023\n",
      "Epoch = 717 Step =    6260 loss = 0.023\n",
      "Epoch = 718 Step =    6270 loss = 0.023\n",
      "Epoch = 719 Step =    6280 loss = 0.023\n",
      "Epoch = 720 Step =    6290 loss = 0.023\n",
      "Epoch = 721 Step =    6300 loss = 0.023\n",
      "Epoch = 722 Step =    6310 loss = 0.023\n",
      "Epoch = 724 Step =    6320 loss = 0.023\n",
      "Epoch = 725 Step =    6330 loss = 0.023\n",
      "Epoch = 726 Step =    6340 loss = 0.023\n",
      "Epoch = 727 Step =    6350 loss = 0.023\n",
      "Epoch = 728 Step =    6360 loss = 0.023\n",
      "Epoch = 729 Step =    6370 loss = 0.023\n",
      "Epoch = 730 Step =    6380 loss = 0.023\n",
      "Epoch = 732 Step =    6390 loss = 0.023\n",
      "Epoch = 733 Step =    6400 loss = 0.023\n",
      "Epoch = 734 Step =    6410 loss = 0.023\n",
      "Epoch = 735 Step =    6420 loss = 0.023\n",
      "Epoch = 736 Step =    6430 loss = 0.023\n",
      "Epoch = 737 Step =    6440 loss = 0.023\n",
      "Epoch = 738 Step =    6450 loss = 0.023\n",
      "Epoch = 740 Step =    6460 loss = 0.022\n",
      "Epoch = 741 Step =    6470 loss = 0.022\n",
      "Epoch = 742 Step =    6480 loss = 0.022\n",
      "Epoch = 743 Step =    6490 loss = 0.022\n",
      "Epoch = 744 Step =    6500 loss = 0.022\n",
      "Epoch = 745 Step =    6510 loss = 0.022\n",
      "Epoch = 746 Step =    6520 loss = 0.022\n",
      "Epoch = 748 Step =    6530 loss = 0.022\n",
      "Epoch = 749 Step =    6540 loss = 0.022\n",
      "Epoch = 750 Step =    6550 loss = 0.022\n",
      "Epoch = 751 Step =    6560 loss = 0.022\n",
      "Epoch = 752 Step =    6570 loss = 0.022\n",
      "Epoch = 753 Step =    6580 loss = 0.022\n",
      "Epoch = 755 Step =    6590 loss = 0.022\n",
      "Epoch = 756 Step =    6600 loss = 0.022\n",
      "Epoch = 757 Step =    6610 loss = 0.022\n",
      "Epoch = 758 Step =    6620 loss = 0.022\n",
      "Epoch = 759 Step =    6630 loss = 0.022\n",
      "Epoch = 760 Step =    6640 loss = 0.022\n",
      "Epoch = 761 Step =    6650 loss = 0.022\n",
      "Epoch = 763 Step =    6660 loss = 0.022\n",
      "Epoch = 764 Step =    6670 loss = 0.022\n",
      "Epoch = 765 Step =    6680 loss = 0.022\n",
      "Epoch = 766 Step =    6690 loss = 0.022\n",
      "Epoch = 767 Step =    6700 loss = 0.022\n",
      "Epoch = 768 Step =    6710 loss = 0.022\n",
      "Epoch = 769 Step =    6720 loss = 0.022\n",
      "Epoch = 771 Step =    6730 loss = 0.022\n",
      "Epoch = 772 Step =    6740 loss = 0.022\n",
      "Epoch = 773 Step =    6750 loss = 0.022\n",
      "Epoch = 774 Step =    6760 loss = 0.022\n",
      "Epoch = 775 Step =    6770 loss = 0.022\n",
      "Epoch = 776 Step =    6780 loss = 0.022\n",
      "Epoch = 777 Step =    6790 loss = 0.022\n",
      "Epoch = 779 Step =    6800 loss = 0.022\n",
      "Epoch = 780 Step =    6810 loss = 0.022\n",
      "Epoch = 781 Step =    6820 loss = 0.022\n",
      "Epoch = 782 Step =    6830 loss = 0.022\n",
      "Epoch = 783 Step =    6840 loss = 0.022\n",
      "Epoch = 784 Step =    6850 loss = 0.022\n",
      "Epoch = 785 Step =    6860 loss = 0.022\n",
      "Epoch = 787 Step =    6870 loss = 0.022\n",
      "Epoch = 788 Step =    6880 loss = 0.022\n",
      "Epoch = 789 Step =    6890 loss = 0.022\n",
      "Epoch = 790 Step =    6900 loss = 0.022\n",
      "Epoch = 791 Step =    6910 loss = 0.021\n",
      "Epoch = 792 Step =    6920 loss = 0.021\n",
      "Epoch = 793 Step =    6930 loss = 0.021\n",
      "Epoch = 795 Step =    6940 loss = 0.021\n",
      "Epoch = 796 Step =    6950 loss = 0.021\n",
      "Epoch = 797 Step =    6960 loss = 0.021\n",
      "Epoch = 798 Step =    6970 loss = 0.021\n",
      "Epoch = 799 Step =    6980 loss = 0.021\n",
      "Epoch = 800 Step =    6990 loss = 0.021\n",
      "Epoch = 801 Step =    7000 loss = 0.021\n",
      "Epoch = 803 Step =    7010 loss = 0.021\n",
      "Epoch = 804 Step =    7020 loss = 0.021\n",
      "Epoch = 805 Step =    7030 loss = 0.021\n",
      "Epoch = 806 Step =    7040 loss = 0.021\n",
      "Epoch = 807 Step =    7050 loss = 0.021\n",
      "Epoch = 808 Step =    7060 loss = 0.021\n",
      "Epoch = 809 Step =    7070 loss = 0.021\n",
      "Epoch = 811 Step =    7080 loss = 0.021\n",
      "Epoch = 812 Step =    7090 loss = 0.021\n",
      "Epoch = 813 Step =    7100 loss = 0.021\n",
      "Epoch = 814 Step =    7110 loss = 0.021\n",
      "Epoch = 815 Step =    7120 loss = 0.021\n",
      "Epoch = 816 Step =    7130 loss = 0.021\n",
      "Epoch = 817 Step =    7140 loss = 0.021\n",
      "Epoch = 819 Step =    7150 loss = 0.021\n",
      "Epoch = 820 Step =    7160 loss = 0.021\n",
      "Epoch = 821 Step =    7170 loss = 0.021\n",
      "Epoch = 822 Step =    7180 loss = 0.021\n",
      "Epoch = 823 Step =    7190 loss = 0.021\n",
      "Epoch = 824 Step =    7200 loss = 0.021\n",
      "Epoch = 825 Step =    7210 loss = 0.021\n",
      "Epoch = 827 Step =    7220 loss = 0.021\n",
      "Epoch = 828 Step =    7230 loss = 0.021\n",
      "Epoch = 829 Step =    7240 loss = 0.021\n",
      "Epoch = 830 Step =    7250 loss = 0.021\n",
      "Epoch = 831 Step =    7260 loss = 0.021\n",
      "Epoch = 832 Step =    7270 loss = 0.021\n",
      "Epoch = 833 Step =    7280 loss = 0.021\n",
      "Epoch = 835 Step =    7290 loss = 0.021\n",
      "Epoch = 836 Step =    7300 loss = 0.021\n",
      "Epoch = 837 Step =    7310 loss = 0.021\n",
      "Epoch = 838 Step =    7320 loss = 0.021\n",
      "Epoch = 839 Step =    7330 loss = 0.021\n",
      "Epoch = 840 Step =    7340 loss = 0.021\n",
      "Epoch = 841 Step =    7350 loss = 0.021\n",
      "Epoch = 843 Step =    7360 loss = 0.021\n",
      "Epoch = 844 Step =    7370 loss = 0.021\n",
      "Epoch = 845 Step =    7380 loss = 0.021\n",
      "Epoch = 846 Step =    7390 loss = 0.021\n",
      "Epoch = 847 Step =    7400 loss = 0.021\n",
      "Epoch = 848 Step =    7410 loss = 0.021\n",
      "Epoch = 849 Step =    7420 loss = 0.021\n",
      "Epoch = 851 Step =    7430 loss = 0.021\n",
      "Epoch = 852 Step =    7440 loss = 0.021\n",
      "Epoch = 853 Step =    7450 loss = 0.020\n",
      "Epoch = 854 Step =    7460 loss = 0.020\n",
      "Epoch = 855 Step =    7470 loss = 0.020\n",
      "Epoch = 856 Step =    7480 loss = 0.020\n",
      "Epoch = 857 Step =    7490 loss = 0.020\n",
      "Epoch = 859 Step =    7500 loss = 0.020\n",
      "Epoch = 860 Step =    7510 loss = 0.020\n",
      "Epoch = 861 Step =    7520 loss = 0.020\n",
      "Epoch = 862 Step =    7530 loss = 0.020\n",
      "Epoch = 863 Step =    7540 loss = 0.020\n",
      "Epoch = 864 Step =    7550 loss = 0.020\n",
      "Epoch = 865 Step =    7560 loss = 0.020\n",
      "Epoch = 867 Step =    7570 loss = 0.020\n",
      "Epoch = 868 Step =    7580 loss = 0.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 869 Step =    7590 loss = 0.020\n",
      "Epoch = 870 Step =    7600 loss = 0.020\n",
      "Epoch = 871 Step =    7610 loss = 0.020\n",
      "Epoch = 872 Step =    7620 loss = 0.020\n",
      "Epoch = 873 Step =    7630 loss = 0.020\n",
      "Epoch = 875 Step =    7640 loss = 0.020\n",
      "Epoch = 876 Step =    7650 loss = 0.020\n",
      "Epoch = 877 Step =    7660 loss = 0.020\n",
      "Epoch = 878 Step =    7670 loss = 0.020\n",
      "Epoch = 879 Step =    7680 loss = 0.020\n",
      "Epoch = 880 Step =    7690 loss = 0.020\n",
      "Epoch = 882 Step =    7700 loss = 0.020\n",
      "Epoch = 883 Step =    7710 loss = 0.020\n",
      "Epoch = 884 Step =    7720 loss = 0.020\n",
      "Epoch = 885 Step =    7730 loss = 0.020\n",
      "Epoch = 886 Step =    7740 loss = 0.020\n",
      "Epoch = 887 Step =    7750 loss = 0.020\n",
      "Epoch = 888 Step =    7760 loss = 0.020\n",
      "Epoch = 890 Step =    7770 loss = 0.020\n",
      "Epoch = 891 Step =    7780 loss = 0.020\n",
      "Epoch = 892 Step =    7790 loss = 0.020\n",
      "Epoch = 893 Step =    7800 loss = 0.020\n",
      "Epoch = 894 Step =    7810 loss = 0.020\n",
      "Epoch = 895 Step =    7820 loss = 0.020\n",
      "Epoch = 896 Step =    7830 loss = 0.020\n",
      "Epoch = 898 Step =    7840 loss = 0.020\n",
      "Epoch = 899 Step =    7850 loss = 0.020\n",
      "Epoch = 900 Step =    7860 loss = 0.020\n",
      "Training is done.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_models\\model.ckpt-5549\n",
      "INFO:tensorflow:Froze 11 variables.\n",
      "INFO:tensorflow:Converted 11 variables to const ops.\n",
      "1683 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Named Entity Recognition \n",
    "\n",
    "        Author : Sangkeun Jung (2017)\n",
    "        - using Tensorflow\n",
    "\"\"\"\n",
    "\n",
    "import sys, os, inspect\n",
    "\n",
    "# add common to path\n",
    "from pathlib import Path\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "common_path = str(Path(currentdir).parent.parent)\n",
    "sys.path.append( common_path )\n",
    "\n",
    "from common.nlp.vocab import Vocab\n",
    "from common.nlp.data_loader import N2NTextData\n",
    "from common.nlp.converter import N2NConverter\n",
    "\n",
    "from dataset import NERDataset\n",
    "from dataset import load_data\n",
    "from common.ml.hparams import HParams\n",
    "\n",
    "import numpy as np\n",
    "import copy \n",
    "import time \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.contrib.layers.python.layers import linear\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.contrib.seq2seq import sequence_loss#sequence의 loss의 평균값을 구할 수있음\n",
    "\n",
    "from common.ml.tf.deploy import freeze_graph\n",
    "\n",
    "\n",
    "\n",
    "print( \"Tensorflow Version : \", tf.__version__)\n",
    "\n",
    "class NER():\n",
    "    def __init__(self, hps, mode=\"train\"):\n",
    "        self.hps = hps\n",
    "        self.x = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_tokens\")\n",
    "        self.y = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_target\")\n",
    "        self.w = tf.placeholder(tf.float32, [None, hps.num_steps], name=\"pl_weight\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name=\"pl_keep_prob\")\n",
    "\n",
    "        ### 4 blocks ###\n",
    "        # 1) embedding\n",
    "        # 2) dropout on input embedding\n",
    "        # 3) sentence encoding using rnn\n",
    "        # 4) bidirectional rnn's output to target classes\n",
    "        # 5) loss calcaulation\n",
    "\n",
    "        def _embedding(x):\n",
    "            # character embedding \n",
    "            shape       = [hps.vocab_size, hps.emb_size]\n",
    "            initializer = tf.initializers.variance_scaling(distribution=\"uniform\", dtype=tf.float32)\n",
    "            emb_mat     = tf.get_variable(\"emb\", shape, initializer=initializer, dtype=tf.float32)\n",
    "            input_emb   = tf.nn.embedding_lookup(emb_mat, x)   # [batch_size, sent_len, emb_dim]\n",
    "\n",
    "            # split input_emb -> num_steps\n",
    "            step_inputs = tf.unstack(input_emb, axis=1)\n",
    "            return step_inputs\n",
    "\n",
    "        def _sequence_dropout(step_inputs, keep_prob):\n",
    "            # apply dropout to each input\n",
    "            # input : a list of input tensor which shape is [None, input_dim]\n",
    "            with tf.name_scope('sequence_dropout') as scope:\n",
    "                step_outputs = []\n",
    "                for t, input in enumerate(step_inputs):\n",
    "                    step_outputs.append( tf.nn.dropout(input, keep_prob) )\n",
    "            return step_outputs\n",
    "\n",
    "        def sequence_encoding_n2n(step_inputs, seq_length, cell_size):\n",
    "            # birnn based N2N encoding and output\n",
    "            f_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=False)\n",
    "            b_rnn_cell = tf.contrib.rnn.GRUCell(cell_size, reuse=False)\n",
    "            _inputs    = tf.stack(step_inputs, axis=1)\n",
    "\n",
    "            # step_inputs = a list of [batch_size, emb_dim]\n",
    "            # input = [batch_size, num_step, emb_dim]\n",
    "            # np.stack( [a,b,c,] )\n",
    "            outputs, states, = tf.nn.bidirectional_dynamic_rnn( f_rnn_cell,\n",
    "                                                                b_rnn_cell,\n",
    "                                                                _inputs,\n",
    "                                                                sequence_length=tf.cast(seq_length, tf.int64),\n",
    "                                                                time_major=False,\n",
    "                                                                dtype=tf.float32,\n",
    "                                                                scope='birnn',\n",
    "                                                            )\n",
    "            output_fw, output_bw = outputs\n",
    "            states_fw, states_bw = states \n",
    "\n",
    "            output       = tf.concat([output_fw, output_bw], 2)\n",
    "            step_outputs = tf.unstack(output, axis=1)\n",
    "\n",
    "            final_state  = tf.concat([states_fw, states_bw], 1)\n",
    "            return step_outputs # a list of [batch_size, enc_dim]\n",
    "\n",
    "        def _to_class_n2n(step_inputs, num_class):\n",
    "            T = len(step_inputs)\n",
    "            step_output_logits = []\n",
    "            for t in range(T):\n",
    "                # encoder to linear(map)\n",
    "                out = step_inputs[t]\n",
    "                if t==0: out = linear(out, num_class, scope=\"Rnn2Target\")\n",
    "                else:    out = linear(out, num_class, scope=\"Rnn2Target\", reuse=True)\n",
    "                step_output_logits.append(out)\n",
    "            return step_output_logits\n",
    "\n",
    "        def _loss(step_outputs, step_refs, weights):\n",
    "            # step_outputs : a list of [batch_size, num_class] float32 - unscaled logits\n",
    "            # step_refs    : [batch_size, num_steps] int32\n",
    "            # weights      : [batch_size, num_steps] float32\n",
    "            # calculate sequence wise loss function using cross-entropy\n",
    "            _batch_output_logits = tf.stack(step_outputs, axis=1)\n",
    "            loss = sequence_loss(\n",
    "                                    logits=_batch_output_logits,        \n",
    "                                    targets=step_refs,\n",
    "                                    weights=weights\n",
    "                                )\n",
    "            return loss\n",
    "        \n",
    "        seq_length    = tf.reduce_sum(self.w, 1) # [batch_size]\n",
    "\n",
    "        step_inputs       = _embedding(self.x)\n",
    "        step_inputs       = _sequence_dropout(step_inputs, self.keep_prob)\n",
    "        step_enc_outputs  = sequence_encoding_n2n(step_inputs, seq_length, hps.enc_dim)\n",
    "        step_outputs      = _to_class_n2n(step_enc_outputs, hps.num_target_class)\n",
    "\n",
    "        self.loss = _loss(step_outputs, self.y, self.w)\n",
    "\n",
    "        # step_preds and step_out_probs\n",
    "        step_out_probs = []\n",
    "        step_out_preds = []\n",
    "        for _output in step_outputs:\n",
    "            _out_probs  = tf.nn.softmax(_output)\n",
    "            _out_pred   = tf.argmax(_out_probs, 1)\n",
    "\n",
    "            step_out_probs.append(_out_probs)\n",
    "            step_out_preds.append(_out_pred)\n",
    "\n",
    "        # stack for interface\n",
    "        self.step_out_probs = tf.stack(step_out_probs, axis=1, name=\"step_out_probs\")\n",
    "        self.step_out_preds = tf.stack(step_out_preds, axis=1, name=\"step_out_preds\")\n",
    "\n",
    "        self.global_step = tf.get_variable(\"global_step\", [], tf.int32, initializer=tf.zeros_initializer, trainable=False)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            optimizer       = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "            self.train_op   = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        else:\n",
    "            self.train_op = tf.no_op()\n",
    "\n",
    "        for v in tf.trainable_variables(): print(v.name)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_hparams():\n",
    "        return HParams(\n",
    "            learning_rate     = 0.005,\n",
    "            keep_prob         = 0.5,\n",
    "        )\n",
    "\n",
    "\n",
    "def train(train_id_data, num_vocabs, num_taget_class):\n",
    "    #\n",
    "    # train sentiment analysis using given train_id_data\n",
    "    #\n",
    "    max_epoch = 900\n",
    "    model_dir = \"./trained_models\"\n",
    "    hps = NER.get_default_hparams()\n",
    "    hps.update(\n",
    "                    batch_size= 100,\n",
    "                    num_steps = 85,\n",
    "                    emb_size  = 40,\n",
    "                    enc_dim   = 100,\n",
    "                    vocab_size=num_vocabs,\n",
    "                    num_target_class=num_taget_class\n",
    "               )\n",
    "\n",
    "    with tf.variable_scope(\"model\"):\n",
    "        model = NER(hps, \"train\")\n",
    "\n",
    "    sv = tf.train.Supervisor(is_chief=True,\n",
    "                             logdir=model_dir,\n",
    "                             summary_op=None,  \n",
    "                             global_step=model.global_step)\n",
    "\n",
    "    # tf assign compatible operators for gpu and cpu \n",
    "    tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "    with sv.managed_session(config=tf_config) as sess:\n",
    "        local_step       = 0\n",
    "        prev_global_step = sess.run(model.global_step)\n",
    "\n",
    "        train_data_set = NERDataset(train_id_data, hps.batch_size, hps.num_steps)\n",
    "        losses = []\n",
    "        while not sv.should_stop():\n",
    "            fetches = [model.global_step, model.loss, model.train_op]\n",
    "            a_batch_data = next( train_data_set.iterator )\n",
    "            y, x, w = a_batch_data\n",
    "            fetched = sess.run(fetches, {\n",
    "                                            model.x: x, \n",
    "                                            model.y: y, \n",
    "                                            model.w: w,\n",
    "\n",
    "                                            model.keep_prob: hps.keep_prob,\n",
    "                                        }\n",
    "                              )\n",
    "\n",
    "            local_step += 1\n",
    "\n",
    "            _global_step = fetched[0]\n",
    "            _loss        = fetched[1]\n",
    "            losses.append( _loss )\n",
    "            if local_step < 10 or local_step % 10 == 0:\n",
    "                epoch = train_data_set.get_epoch_num()\n",
    "                print(\"Epoch = {:3d} Step = {:7d} loss = {:5.3f}\".format(epoch, _global_step, np.mean(losses)) )\n",
    "                _loss = []                \n",
    "                if epoch >= max_epoch : break \n",
    "\n",
    "        print(\"Training is done.\")\n",
    "    sv.stop()\n",
    "\n",
    "    # model.out_pred, model.out_probs\n",
    "    freeze_graph(model_dir, \"model/step_out_preds,model/step_out_probs\", \"frozen_graph.tf.pb\") ## freeze graph with params to probobuf format\n",
    "    \n",
    "from tensorflow.core.framework import graph_pb2\n",
    "def predict(token_vocab, target_vocab, sent):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force to use cpu only (prediction)\n",
    "    model_dir = \"./trained_models\"\n",
    "\n",
    "    # prepare sentence converting\n",
    "    # to make raw sentence to id data easily\n",
    "    pred_data     = N2NTextData(sent, mode='sentence')\n",
    "    pred_id_data  = N2NConverter.convert(pred_data, target_vocab, token_vocab)\n",
    "    pred_data_set = NERDataset(pred_id_data, 1, 85)\n",
    "    #\n",
    "    try:\n",
    "        a_batch_data = next(pred_data_set.predict_iterator) # a result\n",
    "        b_nes_id, b_token_ids, b_weight = a_batch_data\n",
    "    except StopIteration:\n",
    "        print(pred_data_set)\n",
    "\n",
    "    # Restore graph\n",
    "    # note that frozen_graph.tf.pb contains graph definition with parameter values in binary format\n",
    "    _graph_fn =  os.path.join(model_dir, 'frozen_graph.tf.pb')\n",
    "    with tf.gfile.GFile(_graph_fn, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # to check load graph\n",
    "        #for n in tf.get_default_graph().as_graph_def().node: print(n.name)\n",
    "\n",
    "        # make interface for input\n",
    "        pl_token     = graph.get_tensor_by_name('import/model/pl_tokens:0')\n",
    "        pl_weight    = graph.get_tensor_by_name('import/model/pl_weight:0')\n",
    "        pl_keep_prob = graph.get_tensor_by_name('import/model/pl_keep_prob:0')\n",
    "\n",
    "        # make interface for output\n",
    "        step_out_preds = graph.get_tensor_by_name('import/model/step_out_preds:0')\n",
    "        step_out_probs = graph.get_tensor_by_name('import/model/step_out_probs:0')\n",
    "        \n",
    "\n",
    "        # predict sentence \n",
    "        b_best_step_pred_indexs, b_step_pred_probs = sess.run([step_out_preds, step_out_probs], \n",
    "                                                              feed_dict={\n",
    "                                                                            pl_token  : b_token_ids,\n",
    "                                                                            pl_weight : b_weight,\n",
    "                                                                            pl_keep_prob : 1.0,\n",
    "                                                                        }\n",
    "                                                             )\n",
    "        best_step_pred_indexs = b_best_step_pred_indexs[0]\n",
    "        step_pred_probs = b_step_pred_probs[0]\n",
    "\n",
    "        step_best_targets = []\n",
    "        step_best_target_probs = []\n",
    "        for time_step, best_pred_index in enumerate(best_step_pred_indexs):\n",
    "            _target_class = target_vocab.get_symbol(best_pred_index)\n",
    "            step_best_targets.append( _target_class )\n",
    "            _prob = step_pred_probs[time_step][best_pred_index]\n",
    "            step_best_target_probs.append( _prob ) \n",
    "        return sent, step_best_targets\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_id_data, token_vocab, target_vocab = load_data()\n",
    "    num_vocabs       = token_vocab.get_num_tokens()\n",
    "    num_target_class = target_vocab.get_num_targets()\n",
    "\n",
    "    train_data_set = NERDataset(train_id_data, 5, 85)\n",
    "    train(train_id_data, num_vocabs, num_target_class)\n",
    "    \n",
    "    #predict(token_vocab, target_vocab, '아프가니스탄의 장래를 더욱 불투명하게 하는 것은 강경파 헤즈비 이슬라미와 우즈베크 민병대의 대립이다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open('./data/trip.test.txt',encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            sent, targets = predict(token_vocab, target_vocab,line)\n",
    "            print(sent)\n",
    "            print(list(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
